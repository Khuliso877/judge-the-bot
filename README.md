# 🕵️‍♂️ Responsible AI Inspector Report

## Case 1: Hiring Bot Bias

**🧠 What’s happening:**  
A company uses an AI to screen job applicants. It tends to reject female candidates with career gaps.

**⚠️ What’s problematic:**  
- Penalizes caregivers, especially women  
- Trained on biased historical data  
- Lacks transparency and accountability

**✅ One improvement idea:**  
Train the AI to recognize legitimate career gaps (e.g. parental leave) and include human review checkpoints for flagged cases.

---

## Case 2: School Proctoring AI

**🧠 What’s happening:**  
An AI system flags students as cheating based on eye movement and facial behavior.

**⚠️ What’s problematic:**  
- Neurodivergent students are unfairly flagged  
- One-size-fits-all surveillance  
- Raises privacy concerns

**✅ One improvement idea:**  
Use multi-modal behavior analysis and allow opt-out accommodations. Combine AI with teacher judgment and ensure transparency in flag reviews.

---

## 🎨 Bonus Blog Style

> “Career Gap? More Like Algorithmic Trap.”  
> “Eyes Wide Open, Accused of Cheating?”  
> AI isn’t evil—it’s just not always empathetic. As Responsible AI Inspectors, we ask: *Who gets left out? Who gets misunderstood?* Then we fix it. Because tech should serve *everyone*, not just the average.
