Case 1: The Hiring Bot That Ghosts Moms
Title: “Career Gap? More Like Algorithmic Trap.”
What’s happening:
A company uses an AI-powered hiring bot to screen job applications. It scans resumes, filters candidates, and ranks them based on “fit.” But here’s the twist: it tends to reject female applicants who have career gaps—often due to maternity leave or caregiving.
What’s problematic:
This bot isn’t just sorting resumes—it’s silently reinforcing bias. By penalizing career gaps, it indirectly discriminates against women and caregivers. That’s unfair, untransparent, and totally unaccountable. The algorithm might be trained on biased historical data, learning that “continuous employment” equals “better candidate,” without understanding context.
One improvement idea:
Introduce context-aware scoring. Instead of blindly penalizing gaps, the AI should be trained to recognize legitimate reasons (e.g., parental leave, caregiving, illness) and even reward candidates who return to work with upskilling or resilience. Also, include human review checkpoints for flagged cases to ensure fairness.

Case 2: The Proctoring AI That Can’t Read the Room
Title: “Eyes Wide Open, Accused of Cheating?”
What’s happening:
A school uses an AI proctoring system during online exams. It monitors students’ eye movements, facial expressions, and background noise. If someone looks away too often or fidgets, the system flags them as “suspicious.”
What’s problematic:
This system doesn’t account for neurodivergent students—like those with ADHD, autism, or anxiety—who may naturally avoid eye contact or move frequently. It’s a one-size-fits-all surveillance model that punishes difference. Plus, it raises privacy concerns: constant webcam monitoring feels invasive and stressful.
One improvement idea:
Shift to multi-modal behavior analysis that considers diverse learning styles and neurodiversity. Combine AI with teacher judgment and offer opt-out accommodations for students with documented needs. Transparency in how flags are reviewed is key—no one should be penalized for being themselves.
